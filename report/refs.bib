@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}}

@INPROCEEDINGS{hog,
  author={Dalal, N. and Triggs, B.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Histograms of oriented gradients for human detection}, 
  year={2005},
  volume={1},
  number={},
  pages={886-893 vol. 1},
  keywords={Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases},
  doi={10.1109/CVPR.2005.177}}

@INPROCEEDINGS{lbp,
  author={Hadid, Abdenour},
  booktitle={2008 First Workshops on Image Processing Theory, Tools and Applications}, 
  title={The Local Binary Pattern Approach and its Applications to Face Analysis}, 
  year={2008},
  volume={},
  number={},
  pages={1-9},
  keywords={Pattern analysis;Image analysis;Face detection;Face recognition;Image texture analysis;Image recognition;Gray-scale;Biomedical measurements;Biomedical computing;Inspection},
  doi={10.1109/IPTA.2008.4743795}}

@InProceedings{Huang2012a,
  author =    {Gary B. Huang and Marwan Mattar and Honglak Lee and 
               Erik Learned-Miller},
  title =     {Learning to Align from Scratch},
  booktitle = {NIPS},
  year =      {2012}
}

@article{PCA,
title = {Principal components analysis (PCA)},
journal = {Computers & Geosciences},
volume = {19},
number = {3},
pages = {303-342},
year = {1993},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(93)90090-R},
url = {https://www.sciencedirect.com/science/article/pii/009830049390090R},
author = {Andrzej Maćkiewicz and Waldemar Ratajczak},
keywords = {Principal Components Analysis, Variance-covariance matrix, Coefficients of determination, Eigenvalues, Eigenvectors, Correlation matrix, Bartlett's statistics, FORTRAN 77},
abstract = {Principal Components Analysis (PCA) as a method of multivariate statistics was created before the Second World War. However, the wider application of this method only occurred in the 1960s, during the “Quantitative Revolution” in the Natural and Social Sciences. The main reason for this time-lag was the huge difficulty posed by calculations involving this method. Only with the advent and development of computers did the almost unlimited application of multivariate statistical methods, including principal components, become possible. At the same time, requirements arose for precise numerical methods concerning, among other things, the calculation of eigenvalues and eigenvectors, because the application of principal components to technical problems required absolute accuracy. On the other hand, numerous applications in Social Sciences gave rise to a significant increase in the ability to interpret these nonobservable variables, which is just what the principal components are. In the application of principal components, the problem is not only to do with their formal properties but above all, their empirical origins. The authors considered these two tendencies during the creation of the program for principal components. This program—entitled PCA—accompanies this paper. It analyzes consecutively, matrices of variance-covariance and correlations, and performs the following functions: •- the determination of eigenvalues and eigenvectors of these matrices.•- the testing of principal components.•- the calculation of coefficients of determination between selected components and the initial variables, and the testing of these coefficients,•- the determination of the share of variation of all the initial variables in the variation of particular components,•- construction of a dendrite for the initial set of variables,•- the construction of a dendrite for a selected pattern of the principal components,•- the scatter of the objects studied in a selected coordinate system. Thus, the PCA program performs many more functions especially in testing and graphics, than PCA programs in conventional statistical packages. Included in this paper are a theoretical description of principal components, the basic rules for their interpretation and also statistical testing.}
}

@InProceedings{knn,
author="Guo, Gongde
and Wang, Hui
and Bell, David
and Bi, Yaxin
and Greer, Kieran",
editor="Meersman, Robert
and Tari, Zahir
and Schmidt, Douglas C.",
title="KNN Model-Based Approach in Classification",
booktitle="On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="986--996",
abstract="The k-Nearest-Neighbours (kNN) is a simple but effective method for classification. The major drawbacks with respect to kNN are (1) its low efficiency -- being a lazy learning method prohibits it in many applications such as dynamic web mining for a large repository, and (2) its dependency on the selection of a ``good value'' for k. In this paper, we propose a novel kNN type method for classification that is aimed at overcoming these shortcomings. Our method constructs a kNN model for the data, which replaces the data to serve as the basis of classification. The value of k is automatically determined, is varied for different data, and is optimal in terms of classification accuracy. The construction of the model reduces the dependency on k and makes classification faster. Experiments were carried out on some public datasets collected from the UCI machine learning repository in order to test our method. The experimental results show that the kNN based model compares well with C5.0 and kNN in terms of classification accuracy, but is more efficient than the standard kNN.",
isbn="978-3-540-39964-3"
}

@ARTICLE{svm,
  author={Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},
  journal={IEEE Intelligent Systems and their Applications}, 
  title={Support vector machines}, 
  year={1998},
  volume={13},
  number={4},
  pages={18-28},
  keywords={Support vector machines;Machine learning;Algorithm design and analysis;Pattern recognition;Neural networks;Training data;Polynomials;Kernel;Character recognition;Web pages},
  doi={10.1109/5254.708428}}

@ARTICLE{708428,
  author={Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},
  journal={IEEE Intelligent Systems and their Applications}, 
  title={Support vector machines}, 
  year={1998},
  volume={13},
  number={4},
  pages={18-28},
  keywords={Support vector machines;Machine learning;Algorithm design and analysis;Pattern recognition;Neural networks;Training data;Polynomials;Kernel;Character recognition;Web pages},
  doi={10.1109/5254.708428}}

@article{randomforest,
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to },
  added-at = {2015-04-15T08:57:31.000+0200},
  author = {Breiman, Leo},
  biburl = {https://www.bibsonomy.org/bibtex/2b8187107bf870043f2f93669958858f1/kdepublication},
  description = {Random Forests - Springer},
  doi = {10.1023/A:1010933404324},
  interhash = {4450d2e56555e7cb8f3817578e1dd4da},
  intrahash = {b8187107bf870043f2f93669958858f1},
  issn = {0885-6125},
  journal = {Machine Learning},
  keywords = {classification classifier dblp decision ensemble final forest forests imported kde learning machine ml mykopie origin random text-detection the_youtube_social_network thema:exploiting_place_features_in_link_prediction_on_location-based_social_networks trees uw_ss14_web2.0},
  language = {English},
  number = 1,
  pages = {5-32},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2015-04-24T14:37:24.000+0200},
  title = {Random Forests},
  url = {http://dx.doi.org/10.1023/A%3A1010933404324},
  volume = 45,
  year = 2001
}

@article{xgboost,
  author       = {Tianqi Chen and
                  Carlos Guestrin},
  title        = {XGBoost: {A} Scalable Tree Boosting System},
  journal      = {CoRR},
  volume       = {abs/1603.02754},
  year         = {2016},
  url          = {http://arxiv.org/abs/1603.02754},
  eprinttype    = {arXiv},
  eprint       = {1603.02754},
  timestamp    = {Sat, 17 Dec 2022 01:15:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/ChenG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}