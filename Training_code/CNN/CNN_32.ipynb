{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5415876-c254-4dc6-a4bf-1633b9f55d64",
   "metadata": {},
   "source": [
    "# This notebook consists of loading extracted CNN features , applying PCA on it and then some models for testing and then storing in .pkl files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ff3b91-8693-4835-b3c2-6ef2ea28c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08cc1c22-b1ec-4dd2-825b-f6e4e82d3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048eab6d-2c1e-4e44-86b8-a70255043e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e976ad3-6981-4828-b5d4-83a8ab169f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_features = torch.load('extracted_resnet_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cd2113-828a-4130-b0f9-0107b9d64175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X): (13233, 2048)\n",
      "Labels (y): (13233,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class FacesData:\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "    def get_X_y(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        for key, value in self.data_dict.items():\n",
    "            # Extract the label from the file path\n",
    "            label = key.split('/')[1]\n",
    "            X.append(value.flatten())  # Flatten the feature array\n",
    "            y.append(label)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "faces_data = FacesData(cnn_features)\n",
    "X, y = faces_data.get_X_y()\n",
    "\n",
    "print('Features (X):', X.shape)\n",
    "print('Labels (y):', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e81371-5211-45e6-9840-dadff88512f1",
   "metadata": {},
   "source": [
    "### Till above are imports and code for face data using pre extracted CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aadc0333-90fa-482e-b3ea-27d56fe682d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b0b89-eec7-4270-8b3c-c51be2fc1d2c",
   "metadata": {},
   "source": [
    "### Code for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f21dd7b-d7a7-4530-8043-0a819c0d5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the number of features for each split\n",
    "n_features_list = [32, 64, 128]\n",
    "\n",
    "# Dictionary to store the transformed data for each feature set\n",
    "transformed_data_dict = {}\n",
    "# Fit PCA with the maximum number of components needed\n",
    "pca = PCA(n_components=max(n_features_list) )\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into different feature sets\n",
    "for n_features in n_features_list:\n",
    "    transformed_data_dict[n_features] = X_pca[:, :n_features]\n",
    "\n",
    "# Access the transformed data for a specific number of features\n",
    "transformed_data_32 = transformed_data_dict[32]\n",
    "transformed_data_64 = transformed_data_dict[64]\n",
    "transformed_data_128 = transformed_data_dict[128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84c600-c906-46ad-bef7-eaac54e6d686",
   "metadata": {},
   "source": [
    "### Logisitic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64b46dc8-569e-4c4e-ae17-69ae69c8fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "# Train logistic regression for 32 PCA components\n",
    "clf_32 = LogisticRegression(solver='saga', penalty='l2', max_iter=1000, tol=0.01, n_jobs=-1)\n",
    "clf_32.fit(transformed_data_32, y)\n",
    "joblib.dump(clf_32, 'logistic_regression_32_features.pkl')\n",
    "accuracies['32'] = clf_32.score(transformed_data_32, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b8c0d9f-5a29-419a-a2be-dffc2c5c94b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5230106551802313\n"
     ]
    }
   ],
   "source": [
    "print(accuracies['32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d67b9a-2657-4601-80c4-0e65ba893791",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6f50cc6-1710-4f86-b3f7-110f76dfcb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "accuracies_RF = {}\n",
    "# Initialize the Random Forest model\n",
    "# You can adjust n_estimators and max_features to balance between performance and training time\n",
    "rf_model = RandomForestClassifier(n_estimators=10, max_depth = 20 , max_features='sqrt', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the Random Forest model on the dataset\n",
    "# Use transformed_data_32, transformed_data_64, or transformed_data_128 depending on the PCA components you want to use\n",
    "rf_model.fit(transformed_data_32, y)\n",
    "\n",
    "# Save the trained Random Forest model\n",
    "joblib.dump(rf_model, 'random_forest_model_32_features.pkl')\n",
    "# Print accuracy on the training dataset\n",
    "accuracies_RF['32'] = rf_model.score(transformed_data_32, y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97040179-b121-46a5-a25d-530a1ffb20c7",
   "metadata": {},
   "source": [
    "print(accuracies_RF['32'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6d3b477-0b14-4977-8fbe-71df6dfef960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6315272424998111\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_RF['32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410abc1-fc81-47a1-843c-cd0eddaced01",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c3b21b-b3f8-423b-bf1d-fb09398eb45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Assuming transformed_data_32, transformed_data_64, and transformed_data_128 are your feature sets\n",
    "# and 'y' is your target variable with class labels\n",
    "\n",
    "# Encode the class labels in 'y'\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initialize a dictionary to store accuracies for different feature sets\n",
    "accuracies_XGB = {}\n",
    "\n",
    "# Set n_jobs to -1 to use all available cores\n",
    "xgb_model_32 = xgb.XGBClassifier(n_estimators=20, max_depth=20, random_state=42, n_jobs=-1)\n",
    "xgb_model_32.fit(transformed_data_32, y_encoded)\n",
    "joblib.dump(xgb_model_32, 'xgb_model_32_features.pkl')\n",
    "predictions_32 = xgb_model_32.predict(transformed_data_32)\n",
    "accuracies_XGB['32'] = accuracy_score(y_encoded, predictions_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9a613aa-17bd-4506-8ae1-c65aafd65780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210685407692889\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_XGB['32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d498f30-9539-4e39-ba61-60453480905c",
   "metadata": {},
   "source": [
    "### Testing out a few metrics to write in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8da820-fe38-46d9-8831-ee0622567f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Accuracy: 0.5230106551802313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "# Path to the saved model file\n",
    "model_path = 'logistic_regression_32_features.pkl'\n",
    "\n",
    "# Load the logistic regression model from the pickle file\n",
    "clf_32_loaded = joblib.load(model_path)\n",
    "\n",
    "# Use the loaded model to predict the probabilities of the training data\n",
    "probabilities = clf_32_loaded.predict_proba(transformed_data_32)\n",
    "\n",
    "# Evaluate the top-5 accuracy of the model on the training data\n",
    "top_5_accuracy = top_k_accuracy_score(y, probabilities, k=1)\n",
    "print(\"Top-5 Accuracy:\", top_5_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88cb87-8429-450b-964a-00056bd56b2e",
   "metadata": {},
   "source": [
    "### LinearSVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e94f94-fcdf-4268-a5e2-4dc0bc046ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anuj\\miniconda3\\envs\\PRML\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj\\miniconda3\\envs\\PRML\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 796.62 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "accuracies_SVM = {}\n",
    "\n",
    "# Initialize the LinearSVC\n",
    "linear_svm_model = LinearSVC(C=1.0, random_state=42, max_iter=1000)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the LinearSVC model on the dataset with 32 PCA components\n",
    "linear_svm_model.fit(transformed_data_32, y)\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Save the trained SVM model\n",
    "joblib.dump(linear_svm_model, 'linear_svc_32_features.pkl')\n",
    "\n",
    "# Print accuracy on the training dataset\n",
    "accuracies_SVM['32'] = linear_svm_model.score(transformed_data_32, y)\n",
    "\n",
    "# Print the time taken to train the model\n",
    "print(f\"Time taken to fit the model: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37258bd1-5958-40b5-93bb-b3e238e67010",
   "metadata": {},
   "source": [
    "print(accuracies_SVM['32'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3fb6852-78c0-4156-bc64-afbfa6a9758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6727121589964483\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_SVM['32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d77f3-65b3-40b8-bfde-c63a96f71f71",
   "metadata": {},
   "source": [
    "### Testing out metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61b068ec-f423-4c87-b1a7-c38526159476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Top-5 Accuracy: 0.82\n",
      "Regular Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "# Load the trained LinearSVC model\n",
    "linear_svm_model = joblib.load('linear_svc_32_features.pkl')\n",
    "\n",
    "# Assuming 'transformed_data_32' is your feature data and 'y' are your labels\n",
    "decision_scores = linear_svm_model.decision_function(transformed_data_32)\n",
    "sklearn_top5_accuracy = top_k_accuracy_score(y, decision_scores, k=5)\n",
    "print(\"Sklearn Top-5 Accuracy: {:.2f}\".format(sklearn_top5_accuracy))\n",
    "\n",
    "# Regular accuracy\n",
    "predictions = linear_svm_model.predict(transformed_data_32)\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(f\"Regular Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9743d0-efd4-4439-9431-566e611d508b",
   "metadata": {},
   "source": [
    "### Trying out a balanced version of Logistic Regression to check its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9081566c-f401-4757-99c3-c56cc91cd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "# Train logistic regression for 32 PCA components\n",
    "clf_32 = LogisticRegression(solver='saga', penalty='l2', max_iter=1000, tol=0.01, n_jobs=-1 , class_weight = 'balanced')\n",
    "clf_32.fit(transformed_data_32, y)\n",
    "joblib.dump(clf_32, 'balanced_logistic_regression_32_features.pkl')\n",
    "accuracies['32'] = clf_32.score(transformed_data_32, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c3d0742-5221-4d00-b30b-f087c38fbb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5422806619814101\n"
     ]
    }
   ],
   "source": [
    "print(accuracies['32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb041ab-f3c8-445c-bfef-c294224fe85e",
   "metadata": {},
   "source": [
    "### KNN Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4287e65-e672-4cda-9b9a-10f363ab07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "accuracies_KNN = {}\n",
    "\n",
    "# Initialize the KNN model\n",
    "# You can adjust 'n_neighbors' and other parameters to balance between performance and training time\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "\n",
    "# Train the KNN model on the dataset using 32 PCA components\n",
    "# You can change to transformed_data_64 or transformed_data_128 as needed\n",
    "knn_model.fit(transformed_data_32, y)\n",
    "\n",
    "# Save the trained KNN model\n",
    "joblib.dump(knn_model, 'knn_model_32_features.pkl')\n",
    "\n",
    "# Print accuracy on the training dataset\n",
    "accuracies_KNN['32'] = knn_model.score(transformed_data_32, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41447c3b-7f3f-41c7-aa83-9e3e7605390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29486888838509784\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_KNN['32'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a287ac-32f7-4ef1-bb17-fc7ddab8c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Accuracy for KNN_32 : 0.29\n",
      "Top-5 Accuracy for KNN_32: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the trained KNN model\n",
    "knn_model = joblib.load('knn_model_32_features.pkl')  # Update path as needed\n",
    "\n",
    "# Assuming 'transformed_data_32' is your feature data and 'y' are your labels\n",
    "\n",
    "# KNN does not have a decision_function method, but we can use predict_proba for top-k accuracy\n",
    "probabilities = knn_model.predict_proba(transformed_data_32)\n",
    "# Calculate top-k accuracy using the probabilities\n",
    "sklearn_top5_accuracy = top_k_accuracy_score(y, probabilities, k=5)\n",
    "\n",
    "# Regular accuracy using predictions\n",
    "predictions = knn_model.predict(transformed_data_32)\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(f\"Regular Accuracy for KNN_32 : {accuracy:.2f}\")\n",
    "print(f\"Top-5 Accuracy for KNN_32: {sklearn_top5_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69bc6bb3-5795-4660-9f9e-4c5a28aae0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
