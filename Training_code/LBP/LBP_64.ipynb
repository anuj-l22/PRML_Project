{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f0bb8c-4cce-49df-be3e-4297897cb3ad",
   "metadata": {},
   "source": [
    "# This notebook consists of loading extracted LBP features , applying PCA on it and then some models for testing and then storing in .pkl files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f3b5b-aa71-4021-ab31-70bf17924bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33352e-b02f-41b1-ae3e-d7206c3dfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25930af3-53c5-456c-8575-c12b42ea24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp_features = torch.load('extracted_lbp_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77cf941e-4a16-4ac2-9002-964538e99c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X): (13233, 256)\n",
      "Labels (y): (13233,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FacesData:\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "    def get_X_y(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        for key, value in self.data_dict.items():\n",
    "            # Extract the label from the file path\n",
    "            label = key.split('/')[1]\n",
    "            X.append(value.flatten())  # Flatten the feature array\n",
    "            y.append(label)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "faces_data = FacesData(lbp_features)\n",
    "X, y = faces_data.get_X_y()\n",
    "\n",
    "print('Features (X):', X.shape)\n",
    "print('Labels (y):', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5e133-2ae8-44b7-bfe3-d7761b712e72",
   "metadata": {},
   "source": [
    "### Till above are imports and code for face data extraction from LBP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97fd94-8c38-4233-95d4-d2ac73a0ce7b",
   "metadata": {},
   "source": [
    "### Code for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c00bf815-5091-4ed6-8d83-7ea0d5be9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# transformed_data = []\n",
    "# pca = PCA(n_components=16)\n",
    "# X_pca = pca.fit_transform(X)\n",
    "# transformed_data = X_pca\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the number of features for each split\n",
    "n_features_list = [32, 64, 128]\n",
    "\n",
    "# Dictionary to store the transformed data for each feature set\n",
    "transformed_data_dict = {}\n",
    "\n",
    "# Fit PCA with the maximum number of components needed\n",
    "pca = PCA(n_components=max(n_features_list))\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into different feature sets\n",
    "for n_features in n_features_list:\n",
    "    transformed_data_dict[n_features] = X_pca[:, :n_features]\n",
    "\n",
    "# Access the transformed data for a specific number of features\n",
    "transformed_data_32 = transformed_data_dict[32]\n",
    "transformed_data_64 = transformed_data_dict[64]\n",
    "transformed_data_128 = transformed_data_dict[128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4aee79-ac29-4024-94ef-915b4019ec13",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c84b05c6-adae-46de-985c-4fd0c8b1ac10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 37.08 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "accuracies_RF = {}\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=10, max_depth=20, max_features='sqrt', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the Random Forest model on the dataset\n",
    "rf_model.fit(transformed_data_64, y)\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Save the trained Random Forest model\n",
    "joblib.dump(rf_model, 'LBP_random_forest_model_64_features.pkl')\n",
    "\n",
    "# Print accuracy on the training dataset\n",
    "accuracies_RF['64'] = rf_model.score(transformed_data_64, y)\n",
    "\n",
    "# Print the time taken to train the model\n",
    "print(f\"Time taken to fit the model: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cbfca62-0774-499e-88cd-be5bc2d1dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3287236454318749\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_RF['64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1d825-8194-4a5f-b6d3-2c7d7c09ac6a",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c439e0f-2869-42df-a036-9c9c541d5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 247.40 seconds\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import time\n",
    "# Assuming transformed_data_32, transformed_data_64, and transformed_data_128 are your feature sets\n",
    "# and 'y' is your target variable with class labels\n",
    "\n",
    "# Encode the class labels in 'y'\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initialize a dictionary to store accuracies for different feature sets\n",
    "accuracies_XGB = {}\n",
    "\n",
    "# Set n_jobs to -1 to use all available cores\n",
    "xgb_model_64 = xgb.XGBClassifier(n_estimators=20, max_depth=20, random_state=42, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_model_64.fit(transformed_data_64, y_encoded)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "joblib.dump(xgb_model_64, 'LBP_xgb_model_64_features.pkl')\n",
    "\n",
    "predictions_64 = xgb_model_64.predict(transformed_data_64)\n",
    "accuracies_XGB['64'] = accuracy_score(y_encoded, predictions_64)\n",
    "print(f\"Time taken to fit the model: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a2f8b7-77a3-4db6-a97f-33e04c55fed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040051386684803146\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_XGB['64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829a132-9cab-41f1-8420-7cfcf1a253c9",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc4de5c-1321-4085-83da-11f69294819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 3202.17 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import time\n",
    "accuracies = {}\n",
    "\n",
    "# Train logistic regression for 64 PCA components\n",
    "clf_64 = LogisticRegression(solver='saga', penalty='l2', max_iter=1000, tol=0.01, n_jobs=-1)\n",
    "start_time = time.time()\n",
    "clf_64.fit(transformed_data_64, y)\n",
    "elapsed_time = time.time() - start_time\n",
    "joblib.dump(clf_64, 'LBP_logistic_regression_64_features.pkl')\n",
    "accuracies['64'] = clf_64.score(transformed_data_64, y)\n",
    "print(f\"Time taken to fit the model: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c23521-69ee-4080-82e4-0126855bc083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033628051084410186\n"
     ]
    }
   ],
   "source": [
    "print(accuracies['64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f035d28-e78f-45e7-951f-2ed97a7da6ec",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed60fc1-382f-439c-a01c-e181633fbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "accuracies_KNN = {}\n",
    "\n",
    "# Initialize the KNN model\n",
    "# You can adjust 'n_neighbors' and other parameters to balance between performance and training time\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "\n",
    "# Train the KNN model on the dataset using 32 PCA components\n",
    "# You can change to transformed_data_64 or transformed_data_128 as needed\n",
    "knn_model.fit(transformed_data_64, y)\n",
    "\n",
    "# Save the trained KNN model\n",
    "joblib.dump(knn_model, 'LBP_knn_model_64_features.pkl')\n",
    "\n",
    "# Print accuracy on the training dataset\n",
    "accuracies_KNN['64'] = knn_model.score(transformed_data_64, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "637dd8fa-20f3-4050-921b-9f374350de58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21922466560870552\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_KNN['64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866a321-7828-4285-ac46-0b9e31c8cc1b",
   "metadata": {},
   "source": [
    "### LinearSVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fefa4b4-4322-4145-b68e-ce51ca2b549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anuj\\miniconda3\\envs\\PRML\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj\\miniconda3\\envs\\PRML\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 31422.40 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "accuracies_SVM = {}\n",
    "\n",
    "# Initialize the LinearSVC\n",
    "linear_svm_model = LinearSVC(C=1.0, random_state=42, max_iter=1000)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the LinearSVC model on the dataset with 64 PCA components\n",
    "linear_svm_model.fit(transformed_data_64, y)  \n",
    "\n",
    "# Calculate elapsed time\n",
    "end_time = time.time() \n",
    "elapsed_time = end_time - start_time\n",
    "# Save the trained SVM model\n",
    "joblib.dump(linear_svm_model, 'LBP_linear_svc_64_features.pkl')\n",
    "\n",
    "# Print accuracy on the training dataset\n",
    "accuracies_SVM['64'] = linear_svm_model.score(transformed_data_64, y)\n",
    "\n",
    "# Print the time taken to train the model\n",
    "print(f\"Time taken to fit the model: {elapsed_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
