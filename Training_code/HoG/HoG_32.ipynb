{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89a1adf-c05c-4803-b6b0-ef134740023c",
   "metadata": {},
   "source": [
    "# This notebook consists of loading extracted HoG features , applying PCA on it and then some models for testing and then storing in .pkl files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5075856a-6507-4708-aae5-ec60b36e67fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 samples processed.\n",
      "400 samples processed.\n",
      "600 samples processed.\n",
      "800 samples processed.\n",
      "1000 samples processed.\n",
      "1200 samples processed.\n",
      "1400 samples processed.\n",
      "1600 samples processed.\n",
      "1800 samples processed.\n",
      "2000 samples processed.\n",
      "2200 samples processed.\n",
      "2400 samples processed.\n",
      "2600 samples processed.\n",
      "2800 samples processed.\n",
      "3000 samples processed.\n",
      "3200 samples processed.\n",
      "3400 samples processed.\n",
      "3600 samples processed.\n",
      "3800 samples processed.\n",
      "4000 samples processed.\n",
      "4200 samples processed.\n",
      "4400 samples processed.\n",
      "4600 samples processed.\n",
      "4800 samples processed.\n",
      "5000 samples processed.\n",
      "5200 samples processed.\n",
      "5400 samples processed.\n",
      "5600 samples processed.\n",
      "5800 samples processed.\n",
      "6000 samples processed.\n",
      "6200 samples processed.\n",
      "6400 samples processed.\n",
      "6600 samples processed.\n",
      "6800 samples processed.\n",
      "7000 samples processed.\n",
      "7200 samples processed.\n",
      "7400 samples processed.\n",
      "7600 samples processed.\n",
      "7800 samples processed.\n",
      "8000 samples processed.\n",
      "8200 samples processed.\n",
      "8400 samples processed.\n",
      "8600 samples processed.\n",
      "8800 samples processed.\n",
      "9000 samples processed.\n",
      "9200 samples processed.\n",
      "9400 samples processed.\n",
      "9600 samples processed.\n",
      "9800 samples processed.\n",
      "10000 samples processed.\n",
      "10200 samples processed.\n",
      "10400 samples processed.\n",
      "10600 samples processed.\n",
      "10800 samples processed.\n",
      "11000 samples processed.\n",
      "11200 samples processed.\n",
      "11400 samples processed.\n",
      "11600 samples processed.\n",
      "11800 samples processed.\n",
      "12000 samples processed.\n",
      "12200 samples processed.\n",
      "12400 samples processed.\n",
      "12600 samples processed.\n",
      "12800 samples processed.\n",
      "13000 samples processed.\n",
      "13200 samples processed.\n",
      "Features (X): (13233, 70308)\n",
      "Labels (y): (13233,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class FacesData:\n",
    "    def __init__(self, data_paths):\n",
    "        self.data_paths = data_paths\n",
    "        self.data_dict = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        combined_data = {}\n",
    "        for path in self.data_paths:\n",
    "            data = torch.load(path)  # Assuming data is a dictionary as before\n",
    "            combined_data.update(data)\n",
    "        return combined_data\n",
    "\n",
    "    def get_X_y(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        sample_count = 0  # Counter to keep track of the number of samples processed\n",
    "        for key, value in self.data_dict.items():\n",
    "            label = key.split('/')[1]  # Extract the label from the file path\n",
    "            X.append(value.flatten())  # Flatten the feature array\n",
    "            y.append(label)\n",
    "            sample_count += 1\n",
    "            if sample_count % 200 == 0:  # Print progress every 200 samples\n",
    "                print(f\"{sample_count} samples processed.\")\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "# List of paths to your .pt files\n",
    "data_paths = ['extracted_hog_features_0_2000.pt',\n",
    "              'extracted_hog_features_2000_4000.pt',\n",
    "              'extracted_hog_features_4000_6000.pt',\n",
    "              'extracted_hog_features_6000_8000.pt',\n",
    "              'extracted_hog_features_8000_10000.pt',\n",
    "              'extracted_hog_features_10000_12000.pt',\n",
    "              'extracted_hog_features_12000_end.pt']\n",
    "\n",
    "faces_data = FacesData(data_paths)\n",
    "X, y = faces_data.get_X_y()\n",
    "\n",
    "print('Features (X):', X.shape)\n",
    "print('Labels (y):', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da01d102-718a-4c1f-848c-6a1a29c1be28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749,)\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdadb125-3085-4625-b13f-b18cea8dff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Define the dataset, X\n",
    "# Note: You should replace this with your actual dataset loading/preprocessing logic\n",
    "# Example: X = np.random.rand(100, 200)  # 100 samples, 200 features\n",
    "\n",
    "# Define the number of features for each PCA component set\n",
    "n_features_list = [32, 64, 128]\n",
    "\n",
    "# Fit PCA with the maximum number of components needed\n",
    "pca = PCA(n_components=max(n_features_list))\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Store the PCA model\n",
    "joblib.dump(pca, 'pca_model.joblib')\n",
    "\n",
    "# Dictionary to store the transformed data for each feature set\n",
    "transformed_data_dict = {\n",
    "    n_features: X_pca[:, :n_features] for n_features in n_features_list\n",
    "}\n",
    "\n",
    "# Save the transformed features\n",
    "for n_features, data in transformed_data_dict.items():\n",
    "    joblib.dump(data, f'HoG_transformed_data_{n_features}.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc75f44-97b4-466a-9892-83b800d6f518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming y is your labels dataset\n",
    "# Note: You should replace this with your actual labels loading logic\n",
    "# Example: y = np.array([...])\n",
    "\n",
    "# Save the labels\n",
    "joblib.dump(y, 'labels.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a23697-1bce-443e-9b1e-1ac89865c81e",
   "metadata": {},
   "source": [
    "### For the HoG extracted features PCA was applied on them and then the transformed data was saved for future use . Code Above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77094139-810f-4dbd-b7b3-9d9c8a85ea9b",
   "metadata": {},
   "source": [
    "### Loading PCA transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62935747-5bc2-4682-8310-2dd7f503f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PCA model\n",
    "pca_loaded = joblib.load('pca_model.joblib')\n",
    "\n",
    "# Load the transformed data\n",
    "transformed_data_32 = joblib.load('HoG_transformed_data_32.joblib')\n",
    "transformed_data_64 = joblib.load('HoG_transformed_data_64.joblib')\n",
    "transformed_data_128 = joblib.load('HoG_transformed_data_128.joblib')\n",
    "\n",
    "# Now, transformed_data_32, transformed_data_64, and transformed_data_128 can be used as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d0fdb-d357-4520-8fbe-1a8296438e5c",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21aee729-1409-4174-92c0-0473a3758f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit model : 22.662403106689453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "accuracies_RF = {}\n",
    "import time\n",
    "# Initialize the Random Forest model\n",
    "# You can adjust n_estimators and max_features to balance between performance and training time\n",
    "rf_model = RandomForestClassifier(n_estimators=10, max_depth = 20 , max_features='sqrt', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the Random Forest model on the dataset\n",
    "# Use transformed_data_32, transformed_data_64, or transformed_data_128 depending on the PCA components you want to use\n",
    "start_time = time.time()\n",
    "rf_model.fit(transformed_data_32, y)\n",
    "end_time = time.time()\n",
    "elapsed_time =  end_time - start_time\n",
    "print(\"Time taken to fit model :\" ,elapsed_time)\n",
    "# Save the trained Random Forest model\n",
    "joblib.dump(rf_model, 'HoG_random_forest_model_32_features.pkl')\n",
    "# Print accuracy on the training dataset\n",
    "accuracies_RF['32'] = rf_model.score(transformed_data_32, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdefdad0-9ac6-408a-85ff-0c756f95be56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41509861709362955\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_RF['32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242cc2a-fed4-4259-90d3-b67bd7a12da6",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edcf6046-3183-4d0e-8cd8-e82527f42728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit model : 228.61008739471436\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import time\n",
    "# Assuming transformed_data_32, transformed_data_64, and transformed_data_128 are your feature sets\n",
    "# and 'y' is your target variable with class labels\n",
    "\n",
    "# Encode the class labels in 'y'\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initialize a dictionary to store accuracies for different feature sets\n",
    "accuracies_XGB = {}\n",
    "\n",
    "# Set n_jobs to -1 to use all available cores\n",
    "xgb_model_32 = xgb.XGBClassifier(n_estimators=20, max_depth=20, random_state=42, n_jobs=-1)\n",
    "start_time = time.time()\n",
    "xgb_model_32.fit(transformed_data_32, y_encoded)\n",
    "end_time = time.time()\n",
    "elapsed_time =  end_time - start_time\n",
    "print(\"Time taken to fit model :\" ,elapsed_time)\n",
    "joblib.dump(xgb_model_32, 'HoG_xgb_model_32_features.pkl')\n",
    "predictions_32 = xgb_model_32.predict(transformed_data_32)\n",
    "accuracies_XGB['32'] = accuracy_score(y_encoded, predictions_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3701933-6115-49e1-a12f-9b8bf923d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16496637194891559\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_XGB['32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafd6a0-0f86-4efe-8e4f-66f6f65a4833",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5733f788-44e3-407d-b078-789a230898be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit model : 1059.140419960022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import time\n",
    "accuracies = {}\n",
    "\n",
    "# Train logistic regression for 32 PCA components\n",
    "clf_32 = LogisticRegression(solver='saga', penalty='l2', max_iter=1000, tol=0.01, n_jobs=-1)\n",
    "start_time = time.time()\n",
    "clf_32.fit(transformed_data_32, y)\n",
    "end_time = time.time()\n",
    "elapsed_time =  end_time - start_time\n",
    "print(\"Time taken to fit model :\" ,elapsed_time)\n",
    "joblib.dump(clf_32, 'HoG_logistic_regression_32_features.pkl')\n",
    "accuracies['32'] = clf_32.score(transformed_data_32, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c84f90b2-b843-4f70-8cd5-1621b609b170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5748507519081085\n"
     ]
    }
   ],
   "source": [
    "print(accuracies['32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a15379-5e9b-406d-b68b-5a0c046eeb2e",
   "metadata": {},
   "source": [
    "### LinearSVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7375c611-b534-488b-a32c-b90b2f36ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anuj\\miniconda3\\envs\\PRML\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj\\miniconda3\\envs\\PRML\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit model : 1208.382797241211\n",
      "Time taken to fit the model: 1208.38 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "accuracies_SVM = {}\n",
    "\n",
    "# Initialize the LinearSVC\n",
    "linear_svm_model = LinearSVC(C=1.0, random_state=42, max_iter=1000)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the LinearSVC model on the dataset with 32 PCA components\n",
    "linear_svm_model.fit(transformed_data_32, y)\n",
    "\n",
    "# Calculate elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time =  end_time - start_time\n",
    "print(\"Time taken to fit model :\" ,elapsed_time)\n",
    "\n",
    "# Save the trained SVM model\n",
    "joblib.dump(linear_svm_model, 'HoG_linear_svc_32_features.pkl')\n",
    "\n",
    "# Print accuracy on the training dataset\n",
    "accuracies_SVM['32'] = linear_svm_model.score(transformed_data_32, y)\n",
    "\n",
    "# Print the time taken to train the model\n",
    "print(f\"Time taken to fit the model: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bbab602-0d7b-49e0-a282-3ac40b62a724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5842968336733922\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_SVM['32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60be0c-5b1f-43d4-acf5-e666edba8620",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7da3c83-40b4-41e1-b10d-815153b3c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "accuracies_KNN = {}\n",
    "\n",
    "# Initialize the KNN model\n",
    "# You can adjust 'n_neighbors' and other parameters to balance between performance and training time\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "\n",
    "# Train the KNN model on the dataset using 32 PCA components\n",
    "# You can change to transformed_data_64 or transformed_data_128 as needed\n",
    "knn_model.fit(transformed_data_32, y)\n",
    "\n",
    "# Save the trained KNN model\n",
    "joblib.dump(knn_model, 'HoG_knn_model_32_features.pkl')\n",
    "\n",
    "# Print accuracy on the training dataset\n",
    "accuracies_KNN['32'] = knn_model.score(transformed_data_32, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca1d5f2a-edc4-4e46-8f1c-c898be67eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2362276127862163\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_KNN['32'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
