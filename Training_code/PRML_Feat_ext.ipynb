{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOlHSz5lv6b1D9ZrxC5gaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuj-l22/PRML_Project/blob/main/PRML_Feat_ext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The dataset was imported using !wget command and stored in lfw-deepfunneled.tgz and then extracted into lfw-deepfunneled"
      ],
      "metadata": {
        "id": "CiwnW7eRMNZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqJaBQbrACwU"
      },
      "outputs": [],
      "source": [
        "!wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf lfw-deepfunneled.tgz"
      ],
      "metadata": {
        "id": "F9oXLwyJAJe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resnet Features"
      ],
      "metadata": {
        "id": "V3hYayxSqeW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Check if a GPU is available and set the device accordingly\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load pre-trained ResNet-50 model and move it to the device\n",
        "resnet = models.resnet50(pretrained=True).to(device)\n",
        "# Remove the last fully connected layer\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "# Set the model to evaluation mode\n",
        "resnet.eval()\n",
        "\n",
        "# Define a function to extract features from an image\n",
        "def extract_features(image_path, model, device):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = preprocess(image)\n",
        "    # Add batch dimension and move the image to the device\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "    # Extract features\n",
        "    with torch.no_grad():\n",
        "        features = model(image)\n",
        "    # Remove the batch dimension and move the features to the CPU\n",
        "    features = features.squeeze(0).cpu()\n",
        "    return features\n",
        "\n",
        "# Path to the LFW dataset\n",
        "dataset_path = '/content/lfw-deepfunneled'\n",
        "\n",
        "# Dictionary to store features\n",
        "features_dict = {}\n",
        "\n",
        "# Loop through all the images in the dataset\n",
        "for person in os.listdir(dataset_path):\n",
        "    person_path = os.path.join(dataset_path, person)\n",
        "    if os.path.isdir(person_path):\n",
        "        for image_filename in os.listdir(person_path):\n",
        "            image_path = os.path.join(person_path, image_filename)\n",
        "            features = extract_features(image_path, resnet, device)\n",
        "            features_dict[image_path] = features\n",
        "\n",
        "# Save the extracted features to a file\n",
        "torch.save(features_dict, 'extracted_resnet_features.pt')\n"
      ],
      "metadata": {
        "id": "FvJrSrowqdNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HOG"
      ],
      "metadata": {
        "id": "YoC3OagEqkEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage instructions for HoG extraction:\n",
        "### Specify a start index and end index , for example for samples 2000 to 4000 , 4000 to 6000 , etc . This is done to make it computationally possible\n",
        "\n"
      ],
      "metadata": {
        "id": "MN8KTrudOL6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "import torch\n",
        "\n",
        "def compute_hog(img):\n",
        "    resized_img = resize(img, (128 * 4, 64 * 4))\n",
        "    fd, _ = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
        "                cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
        "    return fd\n",
        "\n",
        "dataset_path = 'lfw-deepfunneled'\n",
        "hog_features_dict = {}\n",
        "counter = 0\n",
        "\n",
        "image_files = []\n",
        "for person in os.listdir(dataset_path):\n",
        "    person_path = os.path.join(dataset_path, person)\n",
        "    if os.path.isdir(person_path):\n",
        "        for image_filename in os.listdir(person_path):\n",
        "            image_files.append(os.path.join(person_path, image_filename))\n",
        "\n",
        "# Specify start and end indices here\n",
        "start_index = 4000\n",
        "end_index = 6000  # Adjust this as needed, e.g., 6000 for the second chunk\n",
        "\n",
        "for image_path in image_files[start_index:end_index]:\n",
        "    img = imread(image_path)\n",
        "    fd = compute_hog(img)\n",
        "    hog_features_dict[image_path] = fd\n",
        "    counter += 1\n",
        "    if counter % 100 == 0:\n",
        "        print(f'{counter} HOG samples processed from index {start_index} to {end_index}.')\n",
        "\n",
        "# Save the extracted HOG features to a file\n",
        "save_filename = f'extracted_hog_features_{start_index}_{end_index}.pt'\n",
        "torch.save(hog_features_dict, save_filename)\n",
        "print(f'Features saved to {save_filename}')\n"
      ],
      "metadata": {
        "id": "658GOmKKALzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LBP"
      ],
      "metadata": {
        "id": "EXrDtcBaq-FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def get_pixel(img, center, x, y):\n",
        "    new_value = 0\n",
        "    try:\n",
        "        if img[x][y] >= center:\n",
        "            new_value = 1\n",
        "    except:\n",
        "        pass\n",
        "    return new_value\n",
        "\n",
        "def lbp_calculated_pixel(img, x, y):\n",
        "    center = img[x][y]\n",
        "    val_ar = []\n",
        "    val_ar.append(get_pixel(img, center, x-1, y+1))     # top_right\n",
        "    val_ar.append(get_pixel(img, center, x, y+1))       # right\n",
        "    val_ar.append(get_pixel(img, center, x+1, y+1))     # bottom_right\n",
        "    val_ar.append(get_pixel(img, center, x+1, y))       # bottom\n",
        "    val_ar.append(get_pixel(img, center, x+1, y-1))     # bottom_left\n",
        "    val_ar.append(get_pixel(img, center, x, y-1))       # left\n",
        "    val_ar.append(get_pixel(img, center, x-1, y-1))     # top_left\n",
        "    val_ar.append(get_pixel(img, center, x-1, y))       # top\n",
        "\n",
        "    power_val = [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "    val = 0\n",
        "    for i in range(len(val_ar)):\n",
        "        val += val_ar[i] * power_val[i]\n",
        "    return val\n",
        "\n",
        "\n",
        "def calcLBP(img):\n",
        "    height, width, _ = img.shape\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    img_lbp = np.zeros((height, width), np.uint8)\n",
        "    for i in range(0, height):\n",
        "        for j in range(0, width):\n",
        "            img_lbp[i, j] = lbp_calculated_pixel(img_gray, i, j)\n",
        "    hist_lbp = cv2.calcHist([img_lbp], [0], None, [256], [0, 256])\n",
        "    return hist_lbp.flatten()\n",
        "\n",
        "# Path to the LFW dataset\n",
        "dataset_path = 'content/lfw-deepfunneled'\n",
        "\n",
        "# Dictionary to store LBP features\n",
        "lbp_features_dict = {}\n",
        "\n",
        "# Counter for processed samples\n",
        "counter = 0\n",
        "\n",
        "# Loop through all the images in the dataset\n",
        "for person in os.listdir(dataset_path):\n",
        "    person_path = os.path.join(dataset_path, person)\n",
        "    if os.path.isdir(person_path):\n",
        "        for image_filename in os.listdir(person_path):\n",
        "            image_path = os.path.join(person_path, image_filename)\n",
        "            img = cv2.imread(image_path)\n",
        "            lbp_feature = calcLBP(img)\n",
        "            lbp_features_dict[image_path] = lbp_feature\n",
        "            counter += 1\n",
        "            if counter % 100 == 0:\n",
        "                print(f'{counter} LBP samples processed.')\n",
        "\n",
        "# Save the extracted LBP features to a file\n",
        "torch.save(lbp_features_dict, 'extracted_lbp_features.pt')\n"
      ],
      "metadata": {
        "id": "KyzYwtPFAVhp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
